services:
  namenode:
    image: docker.io/bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"
    environment:
      - CLUSTER_NAME=test
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    volumes:
      - namenode:/hadoop/dfs/name
      - ~/MasterUGR/CCAplicaciones/Practica3/sesionX/data:/data  # Host path : Container path
    networks:
      - hadoop
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  datanode:
    image: docker.io/bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    ports:
      - "9864:9864"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - datanode:/hadoop/dfs/data
    depends_on:
      - namenode
    networks:
      - hadoop
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  spark:
    # Build our custom Python-enabled Spark image
    build: ./spark  # The folder containing your Dockerfile (FROM bitnami/spark...)
    container_name: spark
    # Provide any environment configs Spark might need to locate HDFS
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_MASTER_HOST=spark
      - SPARK_MASTER_PORT=7077
    # We let this container spin up so we can exec in
    ports:
      - "8080:8080"  # Spark master UI
      - "7077:7077"  # Spark master port
    depends_on:
      - namenode
      - datanode
    networks:
      - hadoop

  spark-worker-1:
    build: ./spark
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - spark
    networks:
      - hadoop

  spark-worker-2:
    build: ./spark
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - spark
    networks:
      - hadoop
      
volumes:
  namenode:
  datanode:

networks:
  hadoop:  
